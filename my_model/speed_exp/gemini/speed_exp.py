import torch
import time
from typing import Tuple

# ==========================================
# 1. å¯¼å…¥ä½ çš„æ¨¡å—
# ==========================================
try:
    # å‡è®¾ä½ çš„æ–‡ä»¶åå« hstu_bsa_triton_v2.py
    from hstu_bsa_triton_v2 import HSTU_BSA_Triton
except ImportError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° hstu_bsa_triton_v2.pyï¼Œè¯·æ£€æŸ¥æ–‡ä»¶åï¼")
    exit(1)

def generate_hstu_bsa_inputs(
    batch_size: int, 
    max_seq_len: int, 
    num_heads: int, 
    dim: int,
    device
):
    """
    ä¸“é—¨ä¸º HSTU_BSA_Triton ç”Ÿæˆè¾“å…¥æ•°æ®
    æ–°å¢: g_cmp, g_slc
    """
    # 1. ç”Ÿæˆ Jagged åºåˆ—é•¿åº¦
    lengths = torch.randint(1, max_seq_len + 1, (batch_size,), device=device)
    x_offsets = torch.cat([torch.tensor([0]).to(device), torch.cumsum(lengths, dim=0).to(device)]).to(torch.int32)
    total_L = x_offsets[-1].item()
    
    # 2. æ„é€  Q, K, V (Float16)
    dtype = torch.float16
    q = torch.randn(total_L, num_heads, dim, device=device, dtype=dtype)
    k = torch.randn(total_L, num_heads, dim, device=device, dtype=dtype)
    v = torch.randn(total_L, num_heads, dim, device=device, dtype=dtype)
    
    # 3. [æ–°å¢] æ„é€  Gates (g_cmp, g_slc)
    # å‡è®¾ Gate çš„å½¢çŠ¶æ˜¯ (Total_L, H) æˆ–è€… (Total_L, H, 1)
    # æ ¹æ®ä½ çš„ä»£ç : g_cmp = g_cmp.unsqueeze(-1) å¯çŸ¥è¾“å…¥å¯ä»¥æ˜¯ 2D
    g_cmp = torch.sigmoid(torch.randn(total_L, num_heads, device=device, dtype=dtype))
    g_slc = torch.sigmoid(torch.randn(total_L, num_heads, device=device, dtype=dtype))
    
    return q, k, v, g_cmp, g_slc, x_offsets

def speed_exp(Bsize, max_seq_len, num_heads, emb_dim):
    device = 'cuda:0'
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ° GPU")
        return

    print(f"\nğŸ“Š æµ‹è¯•é…ç½®: [B={Bsize}, L={max_seq_len}, H={num_heads}, D={emb_dim}]")

    # 1. å®ä¾‹åŒ–æ¨¡å‹ (æ ¹æ®ä½ çš„ __init__)
    try:
        # è¿™é‡Œä½ å¯ä»¥è°ƒæ•´ block_size å’Œ block_counts
        model = HSTU_BSA_Triton(block_size=32, block_counts=4).to(device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
        return

    # 2. å‡†å¤‡æ•°æ®
    try:
        q, k, v, g_cmp, g_slc, x_offsets = generate_hstu_bsa_inputs(
            Bsize, max_seq_len, num_heads, emb_dim, device
        )
    except RuntimeError as e:
        print(f"âŒ æ˜¾å­˜ä¸è¶³ (OOM) æ— æ³•ç”Ÿæˆæ•°æ®: {e}")
        return

    # 3. é¢„çƒ­ (Warmup)
    print("   ğŸ”¥ æ­£åœ¨é¢„çƒ­...")
    try:
        for _ in range(5):
            # [å…³é”®ä¿®æ”¹] ä½¿ç”¨æ–°çš„å‚æ•°åˆ—è¡¨è°ƒç”¨ forward
            _ = model(q, k, v, g_cmp, g_slc, x_offsets)
        torch.cuda.synchronize()
    except RuntimeError as e:
        print(f"âŒ é¢„çƒ­å¤±è´¥ (å¯èƒ½å‚æ•°ä¸å¯¹æˆ– OOM): {e}")
        return

    # 4. æ€§èƒ½æµ‹è¯•
    torch.cuda.reset_peak_memory_stats()
    base_mem = torch.cuda.memory_allocated()
    
    start_evt = torch.cuda.Event(enable_timing=True)
    end_evt = torch.cuda.Event(enable_timing=True)
    
    try:
        start_evt.record()
        # === æ ¸å¿ƒè°ƒç”¨ ===
        ret = model(q, k, v, g_cmp, g_slc, x_offsets)
        # ===============
        end_evt.record()
        
        torch.cuda.synchronize()
        
        elapsed_ms = start_evt.elapsed_time(end_evt)
        peak_mem = torch.cuda.max_memory_allocated()
        kernel_overhead = (peak_mem - base_mem) / 1024**2
        
        print(f"   âœ… å®Œæˆ!")
        print(f"      - è€—æ—¶: {elapsed_ms:.3f} ms")
        print(f"      - æ˜¾å­˜å¼€é”€ (Overhead): {kernel_overhead:.2f} MB")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œå´©æºƒ: {e}")

if __name__ == "__main__":
    configs = [
        (32, 256, 8, 128),
        (32, 256, 8, 256),
        (32, 256, 8, 512), # å¤§ Dim æµ‹è¯•
    ]

    for (B, L, H, D) in configs:
        speed_exp(B, L, H, D)