import torch
import time
from typing import Tuple

# ==========================================
# âœ… 1. å¯¼å…¥ä½ çš„æ–°æ¨¡å—
# ==========================================
try:
    from hstu_bsa_triton_v2 import HSTU_BSA_Triton
except ImportError:
    print("âŒ é”™è¯¯: æ‰¾ä¸åˆ° hstu_bsa_triton_v2.pyï¼Œè¯·ç¡®ä¿æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹ï¼")
    exit(1)

def generate_random_jagged_qkv(
    batch_size: int, 
    max_seq_len: int, 
    num_heads: int, 
    dim: int,
    device
) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, int]:
    """
    ç”Ÿæˆç¬¦åˆ HSTU æ ¼å¼çš„ Jagged Tensor æ•°æ®
    æ³¨æ„ï¼šä¸ºäº†æ€§èƒ½å’Œæ˜¾å­˜ä¼˜åŒ–ï¼Œè¿™é‡Œé»˜è®¤ç”Ÿæˆ float16 æ•°æ®
    """
    # éšæœºç”Ÿæˆåºåˆ—é•¿åº¦
    lengths = torch.randint(1, max_seq_len + 1, (batch_size,), device=device)
    # ç”Ÿæˆ Offset
    seq_offsets = torch.cat([torch.tensor([0]).to(device), torch.cumsum(lengths, dim=0).to(device)]).to(torch.int32)
    total_L = seq_offsets[-1].item()
    
    # æ„é€  Q, K, V (ä½¿ç”¨ float16 ä»¥èŠ‚çœæ˜¾å­˜å¹¶ç¬¦åˆ Triton æœ€ä½³å®è·µ)
    dtype = torch.float16 
    q = torch.randn(total_L, num_heads, dim, device=device, dtype=dtype)
    k = torch.randn(total_L, num_heads, dim, device=device, dtype=dtype)
    v = torch.randn(total_L, num_heads, dim, device=device, dtype=dtype)
    
    return q, k, v, seq_offsets, max_seq_len

def speed_exp(Bsize, max_seq_len, num_heads, emb_dim):
    device = 'cuda:0'
    if not torch.cuda.is_available():
        print("âŒ é”™è¯¯: æœªæ£€æµ‹åˆ° GPU")
        return

    ALPHA = 1.0 / (emb_dim ** 0.5)
    
    print(f"\nğŸ“Š æ­£åœ¨æµ‹è¯•é…ç½®: [Batch={Bsize}, SeqLen={max_seq_len}, Heads={num_heads}, Dim={emb_dim}]")
    
    # 1. å®ä¾‹åŒ–æ¨¡å‹
    # å‡è®¾ HSTU_BSA_Triton æ˜¯ä¸€ä¸ª nn.Module æˆ–ç±»ï¼Œä¸éœ€è¦å‚æ•°åˆå§‹åŒ–ï¼Œæˆ–è€…å‚æ•°åœ¨ forward ä¸­
    try:
        model = HSTU_BSA_Triton().to(device)
        # å¦‚æœå®ƒæ˜¯çº¯å‡½æ•°å°è£…ï¼Œä¸éœ€è¦ .to(device)ï¼Œä½†è¿™è¡Œé€šå¸¸ä¸ä¼šæŠ¥é”™
    except Exception as e:
        # å¦‚æœå®ƒä¸æ˜¯ç±»è€Œæ˜¯å‡½æ•°ï¼Œç›´æ¥èµ‹å€¼
        model = HSTU_BSA_Triton

    # 2. å‡†å¤‡æ•°æ®
    try:
        q, k, v, seq_offsets, max_seq_len = generate_random_jagged_qkv(
            Bsize, max_seq_len, num_heads, emb_dim, device
        )
    except RuntimeError as e:
        print(f"âŒ æ•°æ®ç”Ÿæˆé˜¶æ®µæ˜¾å­˜ä¸è¶³ (OOM): {e}")
        return

    # 3. é¢„çƒ­ (Warmup) - è§¦å‘ Triton ç¼–è¯‘
    print("   ğŸ”¥ æ­£åœ¨é¢„çƒ­ (Autotuning Kernel)...")
    try:
        # é¢„çƒ­ 5 æ¬¡
        for _ in range(5):
            # å‡è®¾è°ƒç”¨æ–¹å¼ä¸ä¹‹å‰ä¸€è‡´ã€‚å¦‚æœä½ çš„ forward å‚æ•°ä¸åŒï¼Œè¯·åœ¨è¿™é‡Œä¿®æ”¹
            _ = model(
                N=max_seq_len, 
                alpha=ALPHA, 
                q=q, 
                k=k, 
                v=v, 
                seq_offsets=seq_offsets
            )
        torch.cuda.synchronize()
    except RuntimeError as e:
        if "out of memory" in str(e) or "shared memory" in str(e):
            print(f"âŒ é¢„çƒ­å¤±è´¥: æ˜¾å­˜/å…±äº«å†…å­˜ä¸è¶³ (OOM)ã€‚è¯·å°è¯•å‡å° BLOCK_Mã€‚")
            print(f"   é”™è¯¯è¯¦æƒ…: {e}")
        else:
            print(f"âŒ é¢„çƒ­è¿è¡Œæ—¶é”™è¯¯: {e}")
        return

    # ================= æ ¸å¿ƒæµ‹è¯•åŒº =================
    
    # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
    torch.cuda.reset_peak_memory_stats()
    base_mem = torch.cuda.memory_allocated()
    
    # åˆå§‹åŒ–è®¡æ—¶å™¨
    start_evt = torch.cuda.Event(enable_timing=True)
    end_evt = torch.cuda.Event(enable_timing=True)
    
    try:
        start_evt.record()
        # === æ‰§è¡Œæ¨ç† ===
        ret = model(
            N=max_seq_len, 
            alpha=ALPHA, 
            q=q, 
            k=k, 
            v=v, 
            seq_offsets=seq_offsets
        )
        # ===============
        end_evt.record()
        
        # ç­‰å¾… GPU å®Œæˆ
        torch.cuda.synchronize()
        
        # è®¡ç®—ç»“æœ
        elapsed_ms = start_evt.elapsed_time(end_evt)
        peak_mem = torch.cuda.max_memory_allocated()
        kernel_overhead = (peak_mem - base_mem) / 1024**2
        
        print(f"   âœ… æµ‹è¯•æˆåŠŸ!")
        print(f"      - è€—æ—¶: {elapsed_ms:.3f} ms")
        print(f"      - åŸºç¡€æ˜¾å­˜ (Input): {base_mem / 1024**2:.2f} MB")
        print(f"      - å³°å€¼æ˜¾å­˜ (Total): {peak_mem / 1024**2:.2f} MB")
        print(f"      - Kernelé¢å¤–å¼€é”€:   {kernel_overhead:.2f} MB")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œæµ‹è¯•æ—¶å´©æºƒ: {e}")

if __name__ == "__main__":
    # åœ¨è¿™é‡Œå®šä¹‰ä½ æƒ³æµ‹è¯•çš„æ‰€æœ‰é…ç½®
    # æ ¼å¼: (Batch, SeqLen, Heads, Dim)
    configs_to_test = [
        (32, 256, 8, 64),   # å° Dimï¼ŒåŸºå‡†æµ‹è¯•
        (32, 256, 8, 128),  # å¸¸è§„ Dim
        (32, 256, 8, 256),  # ä¸­ç­‰ Dim (æ³¨æ„ Shared Memory)
        (32, 256, 8, 512),  # å¤§ Dim (å¦‚æœåœ¨ Config ä¸­æ²¡æŠŠ BLOCK_M è®¾ä¸º 16ï¼Œè¿™é‡Œå¯èƒ½ä¼šæŒ‚)
    ]

    print("ğŸš€ å¼€å§‹ HSTU_BSA_Triton æ€§èƒ½æµ‹è¯•...")
    for (B, L, H, D) in configs_to_test:
        speed_exp(B, L, H, D)
        # ç¨å¾®æš‚åœä¸€ä¸‹é‡Šæ”¾èµ„æº
        time.sleep(0.5)